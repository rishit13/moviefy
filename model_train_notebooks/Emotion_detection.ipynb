{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE0fsVpTNm1c"
      },
      "source": [
        "## Emoviefy - A project on recommendation of movies based on expressions of a person"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADPnc7kwN8K4"
      },
      "source": [
        "### Walk through of the project :\r\n",
        "1) Uploading and extracting movie images from a zip file </br>\r\n",
        "2) Extraction of features from using a pretrained model (VGG-16) </br>\r\n",
        "3) Building the top-layer model from prediction </br>\r\n",
        "4) Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F1lRxVGOzzG"
      },
      "source": [
        "### Uploading and extracting movie images from a zip file\r\n",
        "<br>\r\n",
        "THe first step below is to import all the libraries that required for this project. This notebook contains just the training and prediction of images. TO see the development version of the code please refer to the description given in the repository. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_6LWsZO23l_"
      },
      "source": [
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import zipfile\r\n",
        "import tensorflow as tf \r\n",
        "from google.colab import files\r\n",
        "import zipfile\r\n",
        "import tensorflow as tf \r\n",
        "from tensorflow.keras.preprocessing.image import  ImageDataGenerator, load_img, img_to_array, array_to_img\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from matplotlib.image import imread\r\n",
        "import matplotlib.image as mpimg\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "from tensorflow.keras.applications import vgg16\r\n",
        "from tensorflow.keras.layers import Dropout, Dense\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.models import Sequential, load_model\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "from google.colab import files\r\n",
        "from keras.preprocessing import image\r\n",
        "from keras.applications.imagenet_utils import decode_predictions\r\n",
        "import cv2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV2jBTgo1vA8"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG5ALcFuQ-mo"
      },
      "source": [
        "In order to download a datset from kaggle we will be using the kaggle API. For which you will required to download a key from your account on kaggle. Make sure the key is in the root directory. It can be done as below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUMTXEVE2kIA"
      },
      "source": [
        "!mkdir ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd8jVr_P2szZ"
      },
      "source": [
        "!kaggle datasets download -d msambare/fer2013"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaF-D24s2vsr"
      },
      "source": [
        "local_zip  = '/content/fer2013.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lhowhpn2zsQ"
      },
      "source": [
        "def unzip(local_path):\r\n",
        "  zip_ref = zipfile.ZipFile(local_zip,'r')\r\n",
        "  zip_ref.extractall('/content')\r\n",
        "  zip_ref.close()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K7jVeLm3RQf"
      },
      "source": [
        "unzip(local_zip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDPrQqunDWQ9"
      },
      "source": [
        "Data Formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxA4W9LxDU9O"
      },
      "source": [
        "local_path = \"/content/train/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB7kxn-vDU2e"
      },
      "source": [
        "folders = os.listdir(local_path )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ELGFntcGfuK",
        "outputId": "9b22e6a5-9c1c-473d-b572-914c970f2606"
      },
      "source": [
        "folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neutral', 'angry', 'happy', 'sad', 'surprise', 'fear', 'disgust']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VWxq4qMTN4R"
      },
      "source": [
        "This file is mainly used to get the filename and its label in a dataframe format. So that  they can be later utilized in a loop to perform different functions like one hot encoding the labels (for training) and for feature extraction from pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFz95VRDUVR"
      },
      "source": [
        "def gendata(new_path,folders):\r\n",
        "  concatlist = []\r\n",
        "  filename = []\r\n",
        "  category = []\r\n",
        "  print(\"Folder ordering: \" , folders)\r\n",
        "  for index,names in enumerate(folders):\r\n",
        "      sub_dr = new_path + names+'/'\r\n",
        "      for files in os.listdir(new_path + names+'/'):\r\n",
        "        concatlist.append([sub_dr + files,str(index+1)])\r\n",
        "\r\n",
        "  for i in concatlist:\r\n",
        "    filename.append(i[0])\r\n",
        "    category.append(i[1])\r\n",
        "  df = pd.DataFrame({'filename': filename,\r\n",
        "                  'category': category})\r\n",
        "  sns.countplot(x='category',data=df).set_title(\"Data Distribution\")\r\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "civoeFNFTJ-w"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "DMx5hNkUKwUM",
        "outputId": "67fcb6d5-8a16-4af1-f800-4f81a479e74c"
      },
      "source": [
        "train_path = \"/content/train/\"\r\n",
        "test_path = '/content/test/'\r\n",
        "train_df = gendata(train_path,folders)\r\n",
        "test_df  = gendata(test_path,folders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Folder ordering:  ['neutral', 'angry', 'happy', 'sad', 'surprise', 'fear', 'disgust']\n",
            "Folder ordering:  ['neutral', 'angry', 'happy', 'sad', 'surprise', 'fear', 'disgust']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN1zRivL3lsz"
      },
      "source": [
        "def encode_label(data_df):\r\n",
        "  enc = OneHotEncoder(handle_unknown='ignore')\r\n",
        "  enc_df = pd.DataFrame(enc.fit_transform(data_df[['category']]).toarray())\r\n",
        "  data_df = data_df.join(enc_df)\r\n",
        "  return data_df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ero35qTnk42"
      },
      "source": [
        "train_df = encode_label(train_df)\r\n",
        "test_df = encode_label(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR5jwRj1XYsa"
      },
      "source": [
        "The below cell imports the vgg architecture cutting of the top layer. We have considered an input shape of 128 x 128 as that was the best image size that suited for achieving better results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtguTm835FAJ"
      },
      "source": [
        "vgg = vgg16.VGG16(weights='imagenet',\r\n",
        "                  include_top=False,\r\n",
        "                  input_shape=(128,128,3))\r\n",
        "\r\n",
        "for layer in vgg.layers:\r\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDvMTbfS52Ue"
      },
      "source": [
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_3kTX_zamnc"
      },
      "source": [
        "feature_list = []\r\n",
        "feature_list_test = [] \r\n",
        "\r\n",
        "def extract_features(data_df):\r\n",
        "    count = 0\r\n",
        "    datalist = []\r\n",
        "    for path in data_df['filename'].to_numpy():\r\n",
        "        x = load_img(path,target_size=(128,128))\r\n",
        "        img_array = img_to_array(x)\r\n",
        "        img_array = img_array/255.0\r\n",
        "        img_array = np.expand_dims(img_array, axis=0)\r\n",
        "        features = vgg.predict(img_array)\r\n",
        "        count+=1\r\n",
        "        print(count/len(data_df)*100)\r\n",
        "        data_list.append(features)\r\n",
        "    return data_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq7ZiZE_NF8t"
      },
      "source": [
        "feature_list =  extract_features(train_df)\r\n",
        "feature_list_test = extract_features(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R0Ux5IWcASu"
      },
      "source": [
        "Reshaping the features in a flattened format to be passed into the dense neural netowork for prediction. I suggest extracting the features in batches and saving them in your localdrive. Rather than doing it all at once. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2FBXFHCQmlN"
      },
      "source": [
        "feat_lst = np.reshape(feature_list,(-1,4*4*512))\r\n",
        "feat_lst_test = np.reshape(feature_list_test,(-1,4*4*512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOkRESoLuzr7"
      },
      "source": [
        "from tensorflow.keras.regularizers import l2\r\n",
        "input_shape = 4*4*512\r\n",
        "def model(input_shape):\r\n",
        "    model = tf.keras.models.Sequential([ tf.keras.layers.Dense(1024, activation = 'relu',input_shape = (input_shape,)),\r\n",
        "                                        tf.keras.layers.BatchNormalization(),\r\n",
        "                                        tf.keras.layers.Dropout(0.2),\r\n",
        "                                        tf.keras.layers.Dense(512, activation = 'relu', kernel_regularizer= l2(0.01)),\r\n",
        "                                        tf.keras.layers.Dense(256, activation = 'relu', kernel_regularizer= l2(0.01)),\r\n",
        "                                        tf.keras.layers.BatchNormalization(),\r\n",
        "                                        tf.keras.layers.Dropout(0.2),\r\n",
        "                                        tf.keras.layers.Dense(len(os.listdir(train_path)), activation= 'softmax') \r\n",
        "    ])\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNbxJLAzfmJQ"
      },
      "source": [
        "Previously the output paramaeter and encoding were done b thr image data generator but in this case there  since input is passed and features alone we have to code out the y values for calculation of the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WedWaIIo7h9b"
      },
      "source": [
        "y = train_df.iloc[:,2:].to_numpy() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdtC4svi7qs_"
      },
      "source": [
        "y_vval = test_df.iloc[:,2:].to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp-clLE1y3zy",
        "outputId": "c1ff5466-2874-492d-a99a-93fd1a8474fc"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\r\n",
        "model = model(input_shape)\r\n",
        "model.summary()\r\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = [\"acc\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 1024)              8389632   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 7)                 1799      \n",
            "=================================================================\n",
            "Total params: 9,052,679\n",
            "Trainable params: 9,050,119\n",
            "Non-trainable params: 2,560\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSREnsDLzp9C"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs={}):\r\n",
        "    if(logs.get('acc')>0.80):\r\n",
        "      print(\"\\nReached 80% accuracy so cancelling training!\")\r\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqvZy_L92VMj"
      },
      "source": [
        "mycallback = myCallback()\r\n",
        "callbacks = [mycallback]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoxZl9uj2ZPm"
      },
      "source": [
        "steps_per_epoch = len(feat_lst)/64\r\n",
        "validation_steps = len(feat_lst_test)/64\r\n",
        "num_epochs = 90\r\n",
        "\r\n",
        "history = model.fit(feat_lst,y,\r\n",
        "                    epochs=num_epochs,\r\n",
        "                    batch_size = 64,\r\n",
        "                    verbose=1,\r\n",
        "                    steps_per_epoch=steps_per_epoch,\r\n",
        "                    validation_data = ( feat_lst_test,y_vval),\r\n",
        "                    validation_steps=validation_steps,\r\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZOfot9P2s6I"
      },
      "source": [
        "model.save(\"vgg_updated_1_tensor.h5\")\r\n",
        "model.save_weights(\"vgg_weights_updated_2_tensor.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCPPaRS_g6n1"
      },
      "source": [
        "In order for us to see the predictions of saw some images from the web we can used the following code,  that allows us to upload an image and then give us the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t8YXF5avu_Y"
      },
      "source": [
        "uploaded = files.upload()\r\n",
        "#imgl = []\r\n",
        "for fn in uploaded.keys():\r\n",
        " \r\n",
        "  # predicting images\r\n",
        "  path = '/content/' + fn\r\n",
        "  img = image.load_img(path, target_size=(128,128))\r\n",
        "  read_img = img_to_array(img)\r\n",
        "  gray = cv2.cvtColor(np.float32(img), cv2.COLOR_BGR2GRAY)\r\n",
        "  cv2.imwrite(path, gray)\r\n",
        "  read_img = cv2.imread(path)\r\n",
        "  read_img = np.expand_dims(read_img, axis=0) \r\n",
        "  read_img = read_img/255.0\r\n",
        "  vgg_pred = vgg.predict(read_img)\r\n",
        "  vgg_pred = vgg_pred.reshape(1, vgg_pred.shape[1]*vgg_pred.shape[2]*vgg_pred.shape[3])\r\n",
        "  y = model.predict(vgg_pred)\r\n",
        "  y_pred = np.argmax(y, axis=1)\r\n",
        "  print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9QWV6n1Ock-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}